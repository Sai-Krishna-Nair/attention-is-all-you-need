{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UsjM65OiaoCN",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-93e1006ec1e1082c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LlPgKXo1aoCQ",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dc6465894d399755",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vocabularies for English and French\n",
    "# These vocabularies are simplified and may not cover all words in the languages.\n",
    "word_map_en = {\n",
    "    \"<pad\": 0, \"<sos\": 1, \"<eos\": 2, \"<unk\": 3, \"I\": 4, \"you\": 5, \"he\": 6, \"she\": 7, \"it\": 8, \"we\": 9, \"they\": 10, \"my\": 11, \"your\": 12, \"his\": 13, \"her\": 14,\n",
    "    \"our\": 15, \"their\": 16, \"am\": 17, \"is\": 18, \"are\": 19, \"was\": 20, \"were\": 21, \"be\": 22, \"been\": 23, \"have\": 24, \"has\": 25, \"had\": 26, \"do\": 27, \"does\": 28, \"did\": 29,\n",
    "    \"can\": 30, \"could\": 31, \"will\": 32, \"would\": 33, \"should\": 34, \"must\": 35, \"go\": 36, \"goes\": 37, \"went\": 38, \"eat\": 39, \"eats\": 40, \"ate\": 41, \"drink\": 42, \"drinks\": 43, \"drank\": 44,\n",
    "    \"see\": 45, \"sees\": 46, \"saw\": 47, \"read\": 48, \"reads\": 49, \"like\": 50, \"likes\": 51, \"liked\": 52, \"love\": 53, \"loves\": 54, \"loved\": 55, \"hate\": 56, \"hates\": 57, \"hated\": 58, \"want\": 59,\n",
    "    \"wants\": 60, \"wanted\": 61, \"need\": 62, \"needs\": 63, \"make\": 64, \"makes\": 65, \"made\": 66, \"take\": 67, \"takes\": 68, \"took\": 69, \"book\": 70, \"books\": 71, \"car\": 72, \"cars\": 73, \"house\": 74,\n",
    "    \"houses\": 75, \"dog\": 76, \"dogs\": 77, \"cat\": 78, \"cats\": 79, \"man\": 80, \"men\": 81, \"woman\": 82, \"women\": 83, \"child\": 84, \"children\": 85, \"friend\": 86, \"friends\": 87, \"food\": 88, \"water\": 89,\n",
    "    \"day\": 90, \"days\": 91, \"time\": 92, \"school\": 93, \"work\": 94, \"a\": 95, \"an\": 96, \"the\": 97, \"and\": 98, \"but\": 99, \"or\": 100, \"in\": 101, \"on\": 102, \"at\": 103, \"to\": 104,\n",
    "    \"from\": 105, \"with\": 106, \"without\": 107, \"for\": 108, \"of\": 109, \"very\": 110, \"really\": 111, \"not\": 112, \"now\": 113, \"today\": 114, \"yesterday\": 115, \"tomorrow\": 116, \"here\": 117, \"there\": 118, \"good\": 119,\n",
    "    \"bad\": 120, \"big\": 121, \"small\": 122, \"new\": 123, \"old\": 124, \"happy\": 125, \"sad\": 126, \".\": 127, \"?\": 128, \"!\": 129\n",
    "}\n",
    "\n",
    "\n",
    "word_map_fr = {\n",
    "    \"<pad\": 0, \"<sos\": 1, \"<eos\": 2, \"<unk\": 3, \"je\": 4, \"tu\": 5, \"il\": 6, \"elle\": 7, \"on\": 8, \"nous\": 9, \"vous\": 10, \"ils\": 11, \"elles\": 12, \"mon\": 13, \"ma\": 14,\n",
    "    \"mes\": 15, \"ton\": 16, \"ta\": 17, \"tes\": 18, \"son\": 19, \"sa\": 20, \"ses\": 21, \"notre\": 22, \"nos\": 23, \"votre\": 24, \"vos\": 25, \"leur\": 26, \"leurs\": 27, \"suis\": 28, \"es\": 29,\n",
    "    \"est\": 30, \"sommes\": 31, \"êtes\": 32, \"sont\": 33, \"étais\": 34, \"était\": 35, \"être\": 36, \"ai\": 37, \"as\": 38, \"a\": 39, \"avons\": 40, \"avez\": 41, \"ont\": 42, \"avais\": 43, \"avait\": 44,\n",
    "    \"fais\": 45, \"fait\": 46, \"faisons\": 47, \"faites\": 48, \"font\": 49, \"vais\": 50, \"vas\": 51, \"va\": 52, \"allons\": 53, \"allez\": 54, \"vont\": 55, \"mange\": 56, \"manges\": 57, \"mangeons\": 58, \"mangez\": 59,\n",
    "    \"mangent\": 60, \"bois\": 61, \"boit\": 62, \"buvons\": 63, \"buvez\": 64, \"boivent\": 65, \"vois\": 66, \"voit\": 67, \"voyons\": 68, \"voyez\": 69, \"voient\": 70, \"aime\": 71, \"aimes\": 72, \"aimons\": 73, \"aimez\": 74,\n",
    "    \"aiment\": 75, \"veux\": 76, \"veut\": 77, \"voulons\": 78, \"voulez\": 79, \"veulent\": 80, \"livre\": 81, \"livres\": 82, \"voiture\": 83, \"voitures\": 84, \"maison\": 85, \"maisons\": 86, \"chien\": 87, \"chiens\": 88, \"chat\": 89,\n",
    "    \"chats\": 90, \"homme\": 91, \"hommes\": 92, \"femme\": 93, \"femmes\": 94, \"enfant\": 95, \"enfants\": 96, \"ami\": 97, \"amie\": 98, \"amis\": 99, \"amies\": 100, \"nourriture\": 101, \"eau\": 102, \"jour\": 103, \"jours\": 104,\n",
    "    \"temps\": 105, \"école\": 106, \"travail\": 107, \"un\": 108, \"une\": 109, \"le\": 110, \"la\": 111, \"les\": 112, \"et\": 113, \"mais\": 114, \"ou\": 115, \"dans\": 116, \"sur\": 117, \"à\": 118, \"de\": 119,\n",
    "    \"avec\": 120, \"sans\": 121, \"pour\": 122, \"très\": 123, \"vraiment\": 124, \"ne\": 125, \"pas\": 126, \"maintenant\": 127, \"aujourd'hui\": 128, \"hier\": 129, \"demain\": 130, \"ici\": 131, \"là\": 132, \"bon\": 133, \"bonne\": 134,\n",
    "    \"mauvais\": 135, \"mauvaise\": 136, \"grand\": 137, \"grande\": 138, \"petit\": 139, \"petite\": 140, \"nouveau\": 141, \"nouvelle\": 142, \"vieux\": 143, \"vieille\": 144, \"heureux\": 145, \"heureuse\": 146, \"triste\": 147, \".\": 148, \"?\": 149, \"!\": 150\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "def tokenize(sentence,word_map):\n",
    "    return torch.tensor([word_map[word] for word in sentence.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZnQiUqdaoCT"
   },
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "z2-SNJeLaoCU",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d0ff14153214f8c0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model,max_len = 5000):\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "        self.PE = torch.zeros(max_len,d_model)\n",
    "        i = torch.arange(0,d_model,2).float()\n",
    "        token_pos = torch.arange(0,max_len,dtype = torch.float).unsqueeze(1)\n",
    "        d = torch.pow(1000,2*i/d_model)\n",
    "        self.PE[:,0::2] = torch.sin(token_pos/d)\n",
    "        self.PE[:,1::2] = torch.cos(token_pos/d)\n",
    "        self.PE = self.PE.unsqueeze(0)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x+self.PE[:,:x.size(1)]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9PX4WaSaoCW"
   },
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model,num_heads):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model//num_heads\n",
    "        self.query = nn.Linear(d_model,d_model)\n",
    "        self.key = nn.Linear(d_model,d_model)\n",
    "        self.value = nn.Linear(d_model,d_model)\n",
    "        self.fc = nn.Linear(d_model,d_model)\n",
    "\n",
    "    \n",
    "    def forward(self,q_input,k_input,v_input,mask=None):\n",
    "        batch_size,max_len,_ = q_input.size()\n",
    "        Q = self.query(q_input)\n",
    "        K = self.key(q_input)\n",
    "        V= self.value(q_input)\n",
    "        \n",
    "        q = Q.reshape(batch_size,max_len,self.num_heads,self.d_k).transpose(1,2)\n",
    "        k = K.reshape(batch_size,max_len,self.num_heads,self.d_k).transpose(1,2)\n",
    "        v = V.reshape(batch_size,max_len,self.num_heads,self.d_k).transpose(1,2)\n",
    "\n",
    "\n",
    "        attn_scores = torch.matmul(q,k.transpose(-2,-1))/torch.sqrt(torch.tensor(self.d_k,dtype=torch.float))\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask==0,float('-inf'))\n",
    "\n",
    "            \n",
    "        attn_weights = F.softmax(attn_scores,dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights,v).transpose(1,2).contiguous().view(batch_size,max_len,self.d_model)\n",
    "        output = self.fc(attn_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLxiB0LPaoCY"
   },
   "source": [
    "### Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module): \n",
    "    def __init__(self,d_model,hidden,drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward,self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model,hidden)\n",
    "        self.linear2 = nn.Linear(hidden,d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout =  nn.Dropout(p = drop_prob)\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62c6Z6Q4aoCY"
   },
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,ffn_hidden,num_heads,drop_prob):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model = d_model, num_heads = num_heads)\n",
    "        self.drop1 = nn.Dropout(p=drop_prob)\n",
    "        self.norm1 = nn.LayerNorm(d_model,eps = 1e-6)\n",
    "        self.feedforward = PositionwiseFeedForward(d_model = d_model,hidden= ffn_hidden,drop_prob=drop_prob)\n",
    "        self.drop2 = nn.Dropout(p=drop_prob)\n",
    "        self.norm2 = nn.LayerNorm(d_model,1e-6)\n",
    "    def forward(self,x,mask=None):\n",
    "        attn_output = self.mha(x,x,x,mask=None)\n",
    "        attn_output = self.drop1(attn_output)\n",
    "        x = self.norm1(attn_output + x)\n",
    "        ffn_output = self.feedforward(x)\n",
    "        ffn_output = self.drop2(ffn_output)\n",
    "        enc_out = self.norm2(ffn_output + x)\n",
    "        return enc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,d_model,ffn_hidden,num_heads,drop_prob,num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(*[EncoderLayer(d_model,ffn_hidden,num_heads,drop_prob) for _ in range(num_layers)])\n",
    "    def forward(self,x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14i4SZi3aoCZ"
   },
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,ffn_hidden,num_heads,drop_prob):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        self.masked_mha = MultiHeadAttention(d_model = d_model, num_heads = num_heads)\n",
    "        self.drop1 = nn.Dropout(p = drop_prob)\n",
    "        self.norm1 = nn.LayerNorm(d_model,eps = 1e-6)\n",
    "        \n",
    "        self.encoder_decoder_mha = MultiHeadAttention(d_model = d_model, num_heads = num_heads)\n",
    "        self.drop2 = nn.Dropout(p = drop_prob)\n",
    "        self.norm2 = nn.LayerNorm(d_model,eps = 1e-6)\n",
    "        \n",
    "        self.feedforward = PositionwiseFeedForward(d_model = d_model,hidden = ffn_hidden,drop_prob=drop_prob)\n",
    "        self.drop3 = nn.Dropout(p = drop_prob)\n",
    "        self.norm3 = nn.LayerNorm(d_model,eps = 1e-6)\n",
    "\n",
    "    \n",
    "    def forward(self,x,enc_out_k,enc_out_v,decoder_mask):\n",
    "        masked_o = self.masked_mha(x,x,x,mask=decoder_mask)\n",
    "        masked_o = self.drop1(masked_o)\n",
    "        x = self.norm1(x + masked_o)\n",
    "        \n",
    "        cross_attn_o = self.encoder_decoder_mha(x,enc_out_k,enc_out_v,mask = None)\n",
    "        cross_attn_o= self.drop2(cross_attn_o)\n",
    "        x = self.norm2(x + cross_attn_o)\n",
    "        \n",
    "        ffn_output = self.feedforward(x)\n",
    "        ffn_output = self.drop3(ffn_output)\n",
    "        dec_out = self.norm3(x + ffn_output)\n",
    "        \n",
    "        return dec_out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDecoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        x, enc_out_k, enc_out_v, mask = inputs\n",
    "        for module in self._modules.values():\n",
    "            y = module(x, enc_out_k, enc_out_v, mask) #30 x 200 x 512\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
    "                                     for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, enc_out_k, enc_out_v, mask):\n",
    "        x = self.layers(x, enc_out_k, enc_out_v, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Urr7FTUuaoCa"
   },
   "source": [
    "### Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,vocab_size,d_model,num_heads,ffn_hidden,num_encoder_layers,num_decoder_layers,drop_prob=0.2,max_len=5000):\n",
    "        super(Transformer,self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model,max_len)\n",
    "        self.encoder = Encoder(d_model,ffn_hidden,num_heads,drop_prob,num_encoder_layers)\n",
    "        self.decoder = Decoder(d_model,ffn_hidden,num_heads,drop_prob,num_decoder_layers)\n",
    "        self.fc = nn.Linear(d_model,vocab_size)\n",
    "    def forward(self,src,trgt,trgt_mask=None):\n",
    "        src = self.pos_encoder(self.embedding(src))\n",
    "        trgt = self.pos_encoder(self.embedding(trgt))\n",
    "        encoder_o =  self.encoder(src)\n",
    "        decoder_o = self.decoder(trgt,encoder_o,encoder_o,trgt_mask)\n",
    "        output = self.fc(decoder_o)\n",
    "        return output     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8oEPyBVaoCb"
   },
   "source": [
    "### Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "h1yohb8WaoCg",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b78400dfce5aba3f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Prediction \n",
    "def translate(input_sentence, word_map_en, word_map_fr, transformer):\n",
    "    # Tokenize input sentence\n",
    "    input_tensor = tokenize(input_sentence, word_map_en).unsqueeze(0)  # Shape (1, seq_len)\n",
    "    \n",
    "    # Generate a mask for the target sentence\n",
    "    tgt_mask = torch.tril(torch.ones((input_tensor.size(1), input_tensor.size(1)))).unsqueeze(0).unsqueeze(0)  # Lower triangular mask\n",
    "\n",
    "    # Initialize a tensor for the target sentence\n",
    "    target_tensor = torch.zeros((1, input_tensor.size(1)), dtype=torch.long)\n",
    "\n",
    "    # Predict the output sentence (translation)\n",
    "    output = transformer(input_tensor, target_tensor, tgt_mask)\n",
    "\n",
    "    # Apply Softmax to get probabilities\n",
    "    softmax_output = F.softmax(output, dim=-1)\n",
    "\n",
    "    # Get predicted token indices \n",
    "    predicted_tokens = torch.argmax(softmax_output, dim=-1)\n",
    "\n",
    "    # Convert predicted tokens back to words\n",
    "    reverse_word_map_fr = {v: k for k, v in word_map_fr.items()}\n",
    "    translated_sentence = [reverse_word_map_fr[token.item()] for token in predicted_tokens[0] if token != 0]\n",
    "\n",
    "    return \" \".join(translated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5O9nQXpNaoCh"
   },
   "source": [
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tdiK1X_GaoCh",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7396e824a4beef54",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Initialize Model\n",
    "vocab_size_en = len(word_map_en)\n",
    "vocab_size_te = len(word_map_fr)\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2\n",
    "ffn_hidden = 32\n",
    "drop_prob=0.2 \n",
    "max_len=500\n",
    "\n",
    "transformer = Transformer(vocab_size_te, d_model, num_heads, ffn_hidden, num_encoder_layers, num_decoder_layers, drop_prob, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuJVZT1UaoCh"
   },
   "source": [
    "### Take Input Sentence and Generate Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B9kIDvGxaoCh",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0bb5990170158468",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "ffa3ae05-2577-4fff-c546-a5de73dc9c46",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: I am happy .\n",
      "Predicted Translation: veut veut voyons voient\n"
     ]
    }
   ],
   "source": [
    "#Take Input Sentence and Generate Translation\n",
    "input_sentence = \"I am happy .\"  # input sentence\n",
    "predicted_sentence = translate(input_sentence, word_map_en, word_map_fr, transformer)\n",
    "print(\"Input Sentence:\", input_sentence)\n",
    "print(\"Predicted Translation:\", predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
